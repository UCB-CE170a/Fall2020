{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6g_vYQ_B2vu"
   },
   "source": [
    "# Introduction\n",
    "Infrastructures nowadays are generating data at such a rapid rate and great volume that it is sometimes impossible to handle them in traditional ways. Fortunately, programming tools such Python include many powerful libraries that can assist the data wrangling process.\n",
    "\n",
    "In this exercise, you will learn to read and analyze tabular and tree-structured data. Both of which are among the most popular types of inputs you will get for your work or research. Specifically, we will show how to handle `.csv`-like files using the Pandas library (https://pandas.pydata.org/) and perform tasks such as looping through each row and merging two pieces of data based on a common field. The Pandas library also works well with other tabular data sources, such as the `.xlsx` file. On the other hand, you can also work with semi-structured tree-like data (e.g., `.json` or `.xml`) using Python standard libraries (https://docs.python.org/3/library/). Pandas is not a Python standard library, thus it usually requires installation. The `json` and `xml` libaries are included in the Python standard libraries and do not require additional installation.\n",
    "\n",
    "Even though tools such as Pandas have made data analysis and processing a lot easier, in general data processing (cleaning, merging, validation, etc.), or sometimes even making sense of the content in the raw data, can still take the majority of the time in an infrastructure analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SkQm5kBFCOa"
   },
   "outputs": [],
   "source": [
    "### download data\n",
    "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/python-exercises/Day%205/hearst_euclid.csv\" -O hearst_euclid.csv\n",
    "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/python-exercises/Day%205/target_north_berkeley.osm\" -O target_north_berkeley.osm\n",
    "!wget \"https://raw.githubusercontent.com/UCB-CE170a/Fall2020/master/traffic_data/north_berkeley_edges_clean.csv\" -O north_berkeley_edges_clean.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUxBHZh7B2vx"
   },
   "source": [
    "# Tabular data (.csv, .xlsx)\n",
    "You are probably familiar with tabular data from using MS Excel or Google Spreadsheets. If we view each row in the data table as a separate record (e.g., the three rows in the table below represent three separate roads), then all records have the same set of attributes, (e.g., all roads have five attributes: ID, name, direction, start_intersection, and end_intersection). Because of this highly structured format, there is no need to store the name of attributes for each record, which saves a lot of space for large data files compared to `.json` or `.xml` file that will be introduced later.\n",
    "\n",
    "\n",
    "| ID | name          | direction      | start_intersection | end_intersection |\n",
    "|----|---------------|----------------|--------------------|------------------|\n",
    "| 1  | Hearst Avenue | East to West   | La Loma Avenue     | Oxford Street    |\n",
    "| 2  | Hearst Avenue | West to East   | Oxford Street      | La Loma Avenue   |\n",
    "| 3  | Euclid Avenue | South to North | Hearst Avenue      | Marin Avenue     |\n",
    "\n",
    "* Note: record 1 and 2 are the two directions of the same road between two common intersections. Two-way roads like this are usually represented by two separate road links in traffic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Wh4AxJsB2v0"
   },
   "source": [
    "## Reading data\n",
    "In the folder of today's exercise, we provided an example `hearst_euclid.csv` file, which contains some information of the two streets on the Northside of the Berkeley campus. Feel free to download it and open in text editor or Excel while you are doing the exercise below. Now, let's read this file into Python and create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHccE9HxB2v3"
   },
   "outputs": [],
   "source": [
    "# import the pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# read the csv file by file name\n",
    "df = pd.read_csv('hearst_euclid.csv')\n",
    "\n",
    "# print the first few lines of the data\n",
    "### answer here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ph8MVxL1B2wL"
   },
   "source": [
    "## Exploration: shape and unique\n",
    "Now let's do some basic exploration:\n",
    "- How many rows/columns are there in the data?\n",
    "- How many different roads are in the data?\n",
    "\n",
    "Previously, we have read the csv file into the `df` variable, where `df` is short for `dataframe` (think dataframe like an excel table, or the two-dimensional array data frame in software `R`). You can name the variable whatever you like, but `df` is typical for variables that are pandas dataframes.\n",
    "\n",
    "Each pandas dataframe object has the `shape` property, which can be accessed using syntax such as `df.shape`. The first element of the `shape` property is the number of rows in the dataframe, and the second element is the number of columns.\n",
    "\n",
    "To focus on a particular column in a dataframe (which is called a `series`), you can access it using `df[column_name]`. Series has a method called `unique()` that returns the unique values in that column. Notice the `()` after the method, which is not there if we want to access properties such as `shape`.\n",
    "\n",
    "As you will soon find out, our data has two roads and each road has two directions. However, for each road-direction, the road is further broken down into smaller segments, thus the total number of records are larger than four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TerKzKsiB2wN"
   },
   "outputs": [],
   "source": [
    "# print the numbers of rows and columns in the data\n",
    "print( '# rows and columns')\n",
    "### answer here ### \n",
    "\n",
    "# print the names of unique roads in the data\n",
    "print( 'Unique roads: ')\n",
    "### answer here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiBQi73TB2wa"
   },
   "source": [
    "## Selecting data\n",
    "Sometimes, you may want to choose a subset of the data to work with. In this case, you will need to select the data based on some criteria (e.g, give me the `n`-th row or give me all records whose `name` is `Hearst_Ave`). Index in Pandas (as well as in Python) is 0-based, meaning the row id of the first record is 0, the row id of the second record is 1, and so forth. Selecting data in pandas is usually performed by `df.loc[]` (label based) or `df.iloc[]` (integer position based). Or use `df[column_name]` is also handy to retrieve a certain column in the dataframe. We will explain how to use them in the exercise below.\n",
    "\n",
    "If you want to index the data in other ways than introduced below, this page provides further information https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WAiZ5Q-B2wb"
   },
   "outputs": [],
   "source": [
    "# selecting a single row by row id: the 1st row\n",
    "display( df.iloc[0:1] ) \n",
    "\n",
    "# selecting a single row by row id: the 10th row\n",
    "display( df.iloc[?] ) ### change here ###\n",
    "\n",
    "# selecting a single cell row and column id: the 1st column in the first row\n",
    "display( df.iloc[?,?] ) ### change here ###\n",
    "\n",
    "# showing the road name of 41st element (selecting a single cell based on row id and column name)\n",
    "display( df[?].iloc[?] ) ### change here ###\n",
    "\n",
    "# selecting all records of with \"road_name\" == \"Euclid_Ave\" (selecting by condition)\n",
    "# since the subset of the data has many rows, let's just print the first two rows\n",
    "display( df.loc[ ? ].head(2) ) ### change here ###\n",
    "\n",
    "# selecting all records, but only keeping the following columns: 'start_node_id', 'road_name', and 'direction' (selecting multiple columns by column name)\n",
    "display( df[ ? ].head(2) )  ### change here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAgrmxTRB2wl"
   },
   "source": [
    "## Looping through the data\n",
    "In the example data, there are two road names (`Hearst_Ave` and `Euclid_Ave`, while each road has two directions and multiple segments). We now want to create a new column, `road_and_direction`, which combines the first letter in the `road_name` column and `direction` column, e.g., `H_W` for the westward direction of Hearst avenue, or `E_S` for the southward direction of Euclid Avenue.\n",
    "\n",
    "There are different methods to apply the same function to each record in a dataframe. However, for beginners, it is easier to just think of this task as looping through each row of the dataframe (using `iterrows()` or `itertuples()`) and performing the same operations on each row. The performance is usually not worse than more sophisticated methods (unless you are doing simple algebra calculations, which can be vectorized and run much faster with NumPy, see https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6).\n",
    "\n",
    "We will introduce the `itertuples()` method here. It is typically used with a `for` loop. Inside the `for` loop, you can get the value under a specific column of the current row with `getattr()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VY978L-qB2wo"
   },
   "outputs": [],
   "source": [
    "# first, create a new, empty list called `road_and_direction`\n",
    "# this is because usually we don't want to update the dataframe directy in a loop\n",
    "road_and_direction = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    \n",
    "    # get the first letter (remember, the index begins at 0 in Python) of the `road_name` column\n",
    "    r = ?   ### change here ###\n",
    "    d = ?   ### change here ###\n",
    "    r_d = ? ### change here ###  ### you can concatenate strings with the `+` sign\n",
    "    road_and_direction.append( r_d )\n",
    "\n",
    "# create the road_and_direction column\n",
    "df['road_and_direction'] = ?   ### change here ###\n",
    "\n",
    "# check results at random locations\n",
    "display(df.iloc[[0, 1, 50, -2, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPE3NhroB2w5"
   },
   "source": [
    "## Merging\n",
    "You may want to combine two tables into a same dataframe. For example, our table looks like\n",
    "\n",
    "|     | start_node_id  | end_node_id     | road_name   | direction   | road_and_direction |\n",
    "|-----|---------------|----------------|-------------|-------------|--------------------|\n",
    "| 1   | 344          | 345           | Hearst_Ave  | Eastward    | H_E                |\n",
    "| 2   | 345          | 346           | Hearst_Ave  | Eastward    | H_E                |\n",
    "| ..  | ..            | ..             | ..          | ..          | ..                 |\n",
    "\n",
    "\n",
    "We also have another csv file, which looks like\n",
    "\n",
    "|     | start_node_id  | end_node_id     | length | lanes   | maxmph | geometry| .. |\n",
    "|-----|---------------|----------------|--------|---------|--------|---------|----|\n",
    "| 1   | 344          | 345           | 99.814  | 2     | 25.0   |LINESTRING (-122.2732154 37.8733381,-122.2731313 37.8733617,-122.2730898 37.8733733,-122.2729336 37.8733969,-122.272773 37.8734083,-122.2721695 37.8734547,-122.2720926 37.8734606)| .. |\n",
    "| 2   | 345          | 346           | 104.724  | 2     | 25.0   |LINESTRING (-122.2720926 37.8734606,-122.2720131 37.8734689,-122.2718559 37.8734852,-122.2713612 37.8735534,-122.2711421 37.8735819,-122.2709941 37.8736007,-122.2709149 37.8736107)| .. |\n",
    "| ..  | ..            | ..             | ..     | ..      | ..     |         | .. |\n",
    "\n",
    "Note that the two tables have two common column names, `start_igraph` and `end_igraph`. Actually, we also know (since we created the CSV files) that records with the same `start_igraph` and `end_igraph` refer to the same road segment in both CSV files. This makes it easier to combine the two tables into one dataframe. Specifically, we will use the `merge()` method to combine the tables. This (as well as many other pandas functions) works similarly to SQL commands if you are familiar with this database management language.\n",
    "\n",
    "There are four types of join/merge in Pandas. We want to keep all records in the first dataframe, `df`, so we will put it before the second dataframe, `larger_Df`, in the merge function.\n",
    "<img src=\"https://github.com/UCB-CE170a/Fall2020/blob/master/python-exercises/Day%205/types_of_merge.png?raw=1\" alt=\"drawing\" width=\"800\"/>\n",
    "[Source of image](https://www.datasciencemadesimple.com/join-merge-data-frames-pandas-python/)\n",
    "\n",
    "We will learn how to obtain the second csv file during the 7th lecture (September 17).\n",
    "While you are working on the exercise below, try to answer the following questions:\n",
    "- What is the shape of the larger_df?\n",
    "- How many unique street it contains? Note: count unique streets\n",
    "- What do you see if you go to https://www.openstreetmap.org/node/53055204?\n",
    "- What do you see if you go to https://www.openstreetmap.org/way/698994084?\n",
    "- Can you use the above method to quickly locate any road/intersection on OpenStreetMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CiJxH5vB2w7"
   },
   "outputs": [],
   "source": [
    "# read the csv file titled `berkeley_osm_edges.csv`, which has more information about the roads in Berkeley\n",
    "large_df = pd.read_csv('north_berkeley_edges_clean.csv')\n",
    "\n",
    "# check some records\n",
    "display( large_df.loc[ \n",
    "            (large_df['start_node_id'].isin([344, 345])) & \n",
    "            (large_df['end_node_id'].isin([345, 346]))] \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGslctNBB2xF"
   },
   "outputs": [],
   "source": [
    "# combine the two dataframes into a new one called `df_expand`\n",
    "# only keep road segments that are inside `df`\n",
    "df_expand = df.merge( ?, how=?, on=? )   ### change here ###\n",
    "\n",
    "print( df_expand.shape )\n",
    "\n",
    "display( df_expand.head(2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzbZkMrZB2xN"
   },
   "source": [
    "## Grouping statistics\n",
    "Another useful operation in Pandas is to aggregate/group records with a common column value and calculate group statistics. In the exertice below, we will practice how to obtain\n",
    "- The total length of Hearst Avenue and Euclid Avenue (sum up the `length` of segments).\n",
    "- How many segments are there for each road and directionality?\n",
    "\n",
    "We will use the `groupby()` method for this exercise. For more approaches to aggredate data, you can refer to https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw3ImYxzB2xO"
   },
   "outputs": [],
   "source": [
    "# first, group the records based on `road_name` using the `groupby()` method\n",
    "# second, calculate the total `length` of each group using the `agg( )` method on the groupped result\n",
    "display( df_expand.groupby( ? ).agg( ? ) )   ### change here ###\n",
    "\n",
    "# first, group the records that have the same `road_name` and `directionality` using the `groupby()` method\n",
    "# second, count the number of records in each group use the `size()` method on the groupped result\n",
    "# then, use `to_frame()` to give the count column a suitable name\n",
    "display( df_expand.groupby( ? ).size().to_frame( 'count' ) )   ### change here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-2ldPNEB2xX"
   },
   "source": [
    "## Saving files\n",
    "The dataframe can be saved to a CSV file, allowing you to work on it using other software (e.g., Excel, Kepler.gl)\n",
    "- Try dragging the data to https://kepler.gl/demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-zgteCuB2xZ"
   },
   "outputs": [],
   "source": [
    "df_expand.to_csv('euclid_hearst_expand.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG79otASB2xg"
   },
   "source": [
    "# Semi-structured data (.json, .xml, .geojson)\n",
    "JSON (`.json`, `.geojson`) and XML (`.xml`, `.kml`) are also common data file extensions. These types of files have a tree-like hierarchical structure. For example, below is the first few lines of a JSON file containing information of a road network downloaded from the OpenStreetMap's Overpass API (https://overpass-turbo.eu/):\n",
    "```\n",
    "{\n",
    "  \"version\": 0.6,\n",
    "  \"generator\": \"Overpass API\",\n",
    "  \"elements\": [road_1, road_2, ...],\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this section, we will learn some basic parsing of such data. Below are the raw road network data for North Berkeley. The extension is `.osm`, but it is still JSON-like. We will learn how to download such data from OpenStreetMaps in the 7th lecture on September 17.\n",
    "\n",
    "JSON files are read into Python as a dictionary. Try to print out each key (attribute) and value and see what is contained. Try to answer:\n",
    "- Which attribute contains the largest amount of information? Hint: check the type and length of values for each dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zk8tXSsB2xi"
   },
   "outputs": [],
   "source": [
    "# we will handle the `.osm` file using the Python standard library `json`\n",
    "import json\n",
    "\n",
    "# read the file with `json.load( open(file_name) )`\n",
    "osm_data = json.load( open('target_north_berkeley.osm') )\n",
    "\n",
    "# exploration\n",
    "print( 'JSON file (target*.osm) is loaded into Python as: ', ? )   ### change here ###\n",
    "print( 'JSON file has the following keys: ', ? , '\\n' )   ### change here ###\n",
    "print( \"The value of the 'version' key in the dictionary is: \", ? )   ### change here ###\n",
    "print( \"The value of the 'element' key in the dictionary has length: \", ? , '\\n' )   ### change here ###\n",
    "print( \"The first value of the 'element' key in the dictionary is: \", ? )   ### change here ###\n",
    "print( \"The last value of the 'element' key in the dictionary is: \", ? )   ### change here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiXMMGfoB2xp"
   },
   "source": [
    "## Extracting and re-organizing the data\n",
    "After playing around with it, you will possibly realize that the information contained under the 'element' key that we are interested in the most. This part of the data contains the road and intersection information that we need to build a road network. Now let's separate the roads (also known as links/edges/ways) and intersections (also known as nodes). Also, we would only like to keep the following information for roads and intersections:\n",
    "- Intersection: id, lat and lon\n",
    "- Road: id and road name\n",
    "\n",
    "Challenge: can you also extract the start and end coordinates of each road link?\n",
    "Hint: use dictionary lookup.\n",
    "\n",
    "Note: intersections are also called nodes. However, not all nodes in the `.osm` data are intersections. Some nodes that are not a real intersection (e.g., where streets intersect) are used to define the geometry position of, e.g., a curvature or a traffic signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8E4lRpaB2xq"
   },
   "outputs": [],
   "source": [
    "### create an empty dictionary to hold node information\n",
    "all_nodes = {}\n",
    "\n",
    "# this is an example of a road node in the original data\n",
    "print( osm_data['elements'][0], '\\n' )\n",
    "\n",
    "### loop through the record and add new roads to the `all_nodes` dictionary\n",
    "for n in osm_data['elements']:\n",
    "    if n['type']=='node':\n",
    "        all_nodes[n['id']] = (n['lat'], n['lon'])\n",
    "print('The data contains {} nodes'.format(len(all_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIA2En0CB2xz"
   },
   "outputs": [],
   "source": [
    "### create an empty dictionary to hold road link information\n",
    "all_links = {}\n",
    "\n",
    "### this is an example of a road link in the original data\n",
    "print( osm_data['elements'][-1], '\\n')\n",
    "\n",
    "### loop through the record and add new links to the `all_links` dictionary\n",
    "for l in osm_data['elements']:\n",
    "    if (l['type']=='way'):\n",
    "        all_links[l['id']] = (l['id'], l['tags']['highway'])\n",
    "print('it includes {} ways'.format(len(all_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73UdwO7mB2x8"
   },
   "source": [
    "## Saving the data\n",
    "After parsing the JSON data, we would like to save our results to output files. These can be done using the `dump()` method. After this, try opening the output files in a text editor and see if the results are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HZ40H--B2x9"
   },
   "outputs": [],
   "source": [
    "with open('nodes.json', 'w') as outfile:\n",
    "    json.dump(all_nodes, outfile, indent=2)\n",
    "\n",
    "with open('links.json', 'w') as outfile:\n",
    "    json.dump(all_links, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EixNhW2HB2yK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of 05_data_processing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
